{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOXJkj+N7XYHJ8Jc1/lntEP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GHROTH-L/crawler/blob/main/%E7%88%AC%E5%8F%96COVID19%E5%96%AE%E6%97%A5%E7%A2%BA%E8%A8%BA%E4%BA%BA%E6%95%B8%E8%88%87%E6%AD%BB%E4%BA%A1%E6%95%B8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#使用手冊注意\n",
        "\n",
        "\n",
        "1.   發病日與死亡日的日期數請對照一樣\n",
        "2.   要更改發病死亡日請從兩個DEF去做更改\n",
        "3.old 與 new差別在2023/3/20起，配合中央流行疫情指揮中心修訂「嚴重特殊傳染性肺炎」病例定義，該疾病名稱修訂為「嚴重特殊傳染性肺炎(併發症)」，新修訂病例統計資料請於「嚴重特殊傳染性肺炎(併發症)」查詢\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "rLp2LmZaBKSH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 200,
      "metadata": {
        "id": "PqZ7XdXZ0jkL"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "import re\n",
        "import json\n",
        "import pandas as pd\n",
        "from google.colab import files"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#設置查詢年月日與cookies(發病日的)\n",
        "def old_sick(url1):\n",
        "  for_data={\n",
        "    '__RequestVerificationToken': '_gzm0KabA-okYOJq00k92hQy47mD9vOpMrnQMzqq6VkvdT8l4WHpYuX4OCq4cjVtAw-gu3F5-7jeFxVaJREiknNrEMkLNrXqlfnb9dwRFeo1',\n",
        "    'pty_Q': 'N',\n",
        "    'pty_disease': '19CoV',\n",
        "    'pty_period_value': 'ymd', #年月日\n",
        "    'minYear': '2020',\n",
        "    'pty_y_s': '2020',\n",
        "    'pty_m_s': '01',\n",
        "    'pty_d_s': '01',\n",
        "    'pty_y_e': '2023',\n",
        "    'pty_m_e': '05',\n",
        "    'pty_d_e': '31',\n",
        "    'pty_date_type_value': '3',\n",
        "    'pty_immigration_value': ['0,1'], #移入與本土都有\n",
        "    'pty_sickclass_value': 'determined_cnt',\n",
        "    'pty_area_text': '全國',\n",
        "    'pty_city_text': '全部',\n",
        "    'pty_town_text': '全部'\n",
        "\n",
        "    }\n",
        "  cookies={\n",
        "    '__RequestVerificationToken':'phD-ED1aUic2mOrCMtAb5WvEHWuAcOdzbplnhPPELpVaIKnWYigCgthqSahMspq8hCAmgZlgk6b8WjrwZEoFN6774jDGogyYOHp71cdqP9c1',\n",
        "    '_gat_gtag_UA_45325352_1':'1'\n",
        "  }\n",
        "  r1 = requests.post(url1, data= for_data, headers = headers, cookies=cookies)\n",
        "  soup = BeautifulSoup(r1.text,\"html.parser\")\n",
        "  k = soup.find(\"div\", class_=\"detailMain blameChart\").script\n",
        "  script = str(k)\n",
        "  return script"
      ],
      "metadata": {
        "id": "U-9LUfTmn2Zs"
      },
      "execution_count": 201,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#設置查詢年月日與cookies死亡日)\n",
        "def old_death(url2):\n",
        "  for_data2={\n",
        "    '__RequestVerificationToken': '3G_AXFanPFbbHbNrONq0yn-jMMerEuMnB_3xkuSOIdWc87GkJCVJMUQ3evCqyxNeXdS0dKamL10_osVhrraAt8ZGv7yKWtiOrEO4_Z3nYVY1',\n",
        "    'pty_Q': 'N',\n",
        "    'pty_disease': '19CoV',\n",
        "    'pty_period_value': 'ymd',\n",
        "    'minYear': '2020',\n",
        "    'pty_y_s': '2020',\n",
        "    'pty_m_s': '01',\n",
        "    'pty_d_s': '01',\n",
        "    'pty_y_e': '2023',\n",
        "    'pty_m_e': '05',\n",
        "    'pty_d_e': '31',\n",
        "    'pty_date_type_value': '10',\n",
        "    'pty_immigration_value': ['0,1'],\n",
        "    'pty_sickclass_value': 'determined_cnt',\n",
        "    'pty_area_text': '全國',\n",
        "    'pty_city_text': '全部',\n",
        "    'pty_town_text': '全部'\n",
        "\n",
        "  }\n",
        "  cookies2={\n",
        "      '__RequestVerificationToken':'phD-ED1aUic2mOrCMtAb5WvEHWuAcOdzbplnhPPELpVaIKnWYigCgthqSahMspq8hCAmgZlgk6b8WjrwZEoFN6774jDGogyYOHp71cdqP9c1'\n",
        "  }\n",
        "  r2 = requests.post(url2, data= for_data2, headers = headers, cookies=cookies2)\n",
        "  soup2 = BeautifulSoup(r2.text,\"html.parser\")\n",
        "  k2 = soup2.find(\"div\", class_=\"detailMain blameChart\").script\n",
        "  script2 = str(k2)\n",
        "  return script2"
      ],
      "metadata": {
        "id": "q8xcfyYHw0E9"
      },
      "execution_count": 202,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#處理確診發病日的部分 以及死亡日\n",
        "def old_scraping(script,script2):\n",
        "  pattern_categories = r'\"xAxis_categories\":\\[(.*?)\\]' #正規化搜尋\n",
        "  pattern_data = r'\"data\":\\[(.*?)\\]'\n",
        "  '''\n",
        "  前缀表示一个原始字符串（raw string）。使用原始字符串可以防止Python解释器对字符串中的特殊字符进行转义处理。\n",
        "  正则表达式中经常包含许多反斜杠（\\），用于表示特殊字符或元字符。如果不使用原始字符串，那么需要对反斜杠进行转义，例如使用\\\\表示一个反斜杠字符。但是，使用原始字符串时，不需要进行额外的转义，可以直接书写正则表达式模式。\n",
        "  '''\n",
        "  #發病日\n",
        "  matches_categories = re.search(pattern_categories, script)\n",
        "  matches_data = re.search(pattern_data, script)\n",
        "\n",
        "  if matches_categories and matches_data:\n",
        "    categories_str = matches_categories.group(1)\n",
        "    data_str = matches_data.group(1)\n",
        "    \n",
        "    xAxis_categories = [category.strip('\"') for category in categories_str.split(\",\")]\n",
        "    data = [category.strip('\"') for category in data_str.split(\",\")]\n",
        "  #死亡日\n",
        "  matches_categories2 = re.search(pattern_categories, script2)\n",
        "  matches_data2 = re.search(pattern_data, script2)    \n",
        "  if matches_categories2 and matches_data2:\n",
        "    categories_str2 = matches_categories2.group(1)\n",
        "    data_str2 = matches_data2.group(1)\n",
        "    \n",
        "    xAxis_categories2 = [category.strip('\"') for category in categories_str2.split(\",\")]\n",
        "    data2 = [category.strip('\"') for category in data_str2.split(\",\")]\n",
        "    #很奇怪爬取的資料 死亡數會少20200229這一天 不過這一天是0\n",
        "    xAxis_categories2.insert(59,'20200229')\n",
        "    data2.insert(59,'0')\n",
        "    #print(\"xAxis_categories:\", xAxis_categories)\n",
        "    #print(\"data:\", data)\n",
        "    #print(\"data:\", dat2)\n",
        "  return [xAxis_categories, data, data2]"
      ],
      "metadata": {
        "id": "b9iSH7SE3ktV"
      },
      "execution_count": 203,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#創造字典 轉 輸出csv\n",
        "def all_old_data(xAxis_categories,data,data2):\n",
        "  all_old_data ={\n",
        "      \"日期\":xAxis_categories,\n",
        "      \"確診人數(發病日)\":data,\n",
        "      \"死亡人數(死亡日)\":data2\n",
        "\n",
        "  }\n",
        "  df= pd.DataFrame(all_old_data)\n",
        "\n",
        "  # 保存DataFrame为CSV文件\n",
        "  df.to_csv('all_old_data.csv', index=False)\n",
        "\n",
        "  # 下载CSV文件\n",
        "  files.download('all_old_data.csv')"
      ],
      "metadata": {
        "id": "HW6YpRTYErLL"
      },
      "execution_count": 204,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#設置查詢年月日與cookies(發病日的)\n",
        "def new_sick(url3):\n",
        "  for_data={\n",
        "    '__RequestVerificationToken': 'xWV8VwKfk6v8Wag6BeqZDa9FAZD7DH-ZQD2ttWgCee-bOA89p6pyttzTgIUWvlGjYEWE6L6ihF07-uar3OLG_QGr_92eiNuuuBrBxLlZwuQ1',\n",
        "    'pty_Q': 'N',\n",
        "    'pty_disease': '19CVS',\n",
        "    'pty_period_value': 'ymd', #年月日\n",
        "    'minYear': '2023',\n",
        "    'pty_y_s': '2023',\n",
        "    'pty_m_s': '01',\n",
        "    'pty_d_s': '01',\n",
        "    'pty_y_e': '2023',\n",
        "    'pty_m_e': '05',\n",
        "    'pty_d_e': '31',\n",
        "    'pty_date_type_value': '3',\n",
        "    'pty_immigration_value': ['0,1'], #移入與本土都有\n",
        "    'pty_sickclass_value': 'determined_cnt',\n",
        "    'pty_area_text': '全國',\n",
        "    'pty_city_text': '全部',\n",
        "    'pty_town_text': '全部'\n",
        "\n",
        "    }\n",
        "  cookies3={\n",
        "    '__RequestVerificationToken':'phD-ED1aUic2mOrCMtAb5WvEHWuAcOdzbplnhPPELpVaIKnWYigCgthqSahMspq8hCAmgZlgk6b8WjrwZEoFN6774jDGogyYOHp71cdqP9c1',\n",
        "    '_ga_KB9TE8J28D':'GS1.1.1685524938.4.1.1685524957.0.0.0',\n",
        "    'Guide_Disease':'true'\n",
        "  }\n",
        "  r3 = requests.post(url3, data= for_data, headers = headers, cookies=cookies3)\n",
        "  soup3 = BeautifulSoup(r3.text,\"html.parser\")\n",
        "  k3 = soup3.find(\"div\", class_=\"detailMain blameChart\").script\n",
        "  script3 = str(k3)\n",
        "  return script3"
      ],
      "metadata": {
        "id": "pzsUinxDRe5u"
      },
      "execution_count": 205,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#處理確診發病日\n",
        "def new_scraping(script3):\n",
        "  pattern_categories = r'\"xAxis_categories\":\\[(.*?)\\]' #正規化搜尋\n",
        "  pattern_data = r'\"data\":\\[(.*?)\\]'\n",
        "  #發病日\n",
        "  matches_categories3 = re.search(pattern_categories, script3)\n",
        "  matches_data3 = re.search(pattern_data, script3)\n",
        "\n",
        "  if matches_categories3 and matches_data3:\n",
        "    categories_str3 = matches_categories3.group(1)\n",
        "    data_str3 = matches_data3.group(1)\n",
        "    \n",
        "    xAxis_categories3 = [category.strip('\"') for category in categories_str3.split(\",\")]\n",
        "    data3 = [category.strip('\"') for category in data_str3.split(\",\")]\n",
        "  return [xAxis_categories3, data3]"
      ],
      "metadata": {
        "id": "A7Qvyxd8UBi3"
      },
      "execution_count": 206,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#創造字典 轉 輸出csv\n",
        "def all_new_data(xAxis_categories3,data3):\n",
        "  all_new_data ={\n",
        "      \"日期\":xAxis_categories3,\n",
        "      \"確診人數(發病日)\":data3\n",
        "  }\n",
        "  df= pd.DataFrame(all_new_data)\n",
        "\n",
        "  # 保存DataFrame为CSV文件\n",
        "  df.to_csv('all_new_data.csv', index=False)\n",
        "\n",
        "  # 下载CSV文件\n",
        "  files.download('all_new_data.csv')"
      ],
      "metadata": {
        "id": "wTW3PJCuUvA3"
      },
      "execution_count": 207,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "  url1 = 'https://nidss.cdc.gov.tw/nndss/disease?id=19CoV' #舊的發病日\n",
        "  url2 = 'https://nidss.cdc.gov.tw/nndss/deadmap?id=19CoV&type=3' #舊的死亡日\n",
        "  url3 = 'https://nidss.cdc.gov.tw/nndss/disease?id=19CVS' #新的發病日\n",
        "  headers = {\"user-agen\" :\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/113.0.0.0 Safari/537.36\"}\n",
        "  print(\"爬取確診資料\")\n",
        "  script = old_sick(url1)\n",
        "  time.sleep(10)\n",
        "  print(\"爬取死亡資料\")\n",
        "  script2 = old_death(url2)\n",
        "  time.sleep(10)\n",
        "  print(\"爬取新定義的資料\")\n",
        "  script3 = new_sick(url3)\n",
        "  old_scrap = old_scraping(script,script2)\n",
        "  new_scrap = new_scraping(script3)\n",
        "  all_old_data(old_scrap[0],old_scrap[1],old_scrap[2])\n",
        "  all_new_data(new_scrap[0],new_scrap[1])\n",
        "  \n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "T73oJC-tuDWM",
        "outputId": "939fcd98-3349-46ea-8984-8f7772a45a28"
      },
      "execution_count": 208,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "爬取確診資料\n",
            "爬取死亡資料\n",
            "爬取新定義的資料\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_1095c5ce-13c0-46c7-9932-50b6ae7adfda\", \"all_old_data.csv\", 18341)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_dd58a823-ed96-4abe-8498-fbcb94aff82e\", \"all_new_data.csv\", 1818)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8UP0EocjZsoT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}